{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tick, given_treecomplexity, given_learningrate, given_subsample):\n",
    "    #risk factors\n",
    "    risk_factor_data=pd.read_excel(r'input/tick_risk_factor.xlsx')\n",
    "    risk_factor_data['id']=risk_factor_data['id'].astype(str)\n",
    "    risk_factor_data['country']=risk_factor_data['country'].astype(str)\n",
    "    risk_factor_data['province']=risk_factor_data['province'].astype(str)\n",
    "    risk_factor_data['county']=risk_factor_data['county'].astype(str)\n",
    "    risk_factors=risk_factor_data.columns[4:]\n",
    "\n",
    "    \n",
    "    #tick data\n",
    "    tick_data=pd.read_excel(r'input/tick_county_distribution.xlsx')\n",
    "    tick_data=tick_data[['id', 'country', 'province', 'county', tick]]\n",
    "    tick_data['id']=tick_data['id'].astype(str)\n",
    "    tick_data['country']=tick_data['country'].astype(str)\n",
    "    tick_data['province']=tick_data['province'].astype(str)\n",
    "    tick_data['county']=tick_data['county'].astype(str)\n",
    "    \n",
    "    #combine and split\n",
    "    complete_data=risk_factor_data.merge(tick_data, left_on=['id', 'country', 'province', 'county'], right_on=['id', 'country', 'province', 'county'], how='left')\n",
    "    complete_data[tick]=complete_data[tick].fillna(-999)\n",
    "    survey_data=complete_data[complete_data[tick]!=-999].reset_index(drop=True)\n",
    "    # print(survey_data.shape[0])\n",
    "\n",
    "    #add weight\n",
    "    weight=pd.read_excel(r'input/weight.xlsx')\n",
    "    weight['id']=weight['id'].astype(str)\n",
    "    weight['country']=weight['country'].astype(str)\n",
    "    weight['province']=weight['province'].astype(str)\n",
    "    weight['county']=weight['county'].astype(str)\n",
    "    weight=weight[['id', 'country', 'province', 'county', 'rescaled']]\n",
    "    # print(weight.shape[0])\n",
    "\n",
    "    survey_data=survey_data.merge(weight, left_on=['id', 'country', 'province', 'county'], right_on=['id', 'country', 'province', 'county'], how='left')\n",
    "    survey_data[tick]=survey_data[tick].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    #step 1, identify key risk factors\n",
    "    rel_con=pd.DataFrame({'Key': risk_factors})   \n",
    "    for i in range(10):\n",
    "            train, test = train_test_split(survey_data, test_size=0.25,  random_state=i)\n",
    "            while test[test[tick]==1].shape[0]<1 or train[train[tick]==1].shape[0]<1:\n",
    "                train, test = train_test_split(survey_data, test_size=0.25)\n",
    "            train_x = train[risk_factors]\n",
    "            train_y = train[tick]\n",
    "            train_weight =  train['rescaled']\n",
    "            model = xgb.XGBClassifier(objective='binary:logistic', learning_rate=given_learningrate, max_depth=given_treecomplexity, subsample=given_subsample, n_estimators=3000,  random_state=i) # colsample_bytree=1\n",
    "            model.fit(train_x, train_y, sample_weight=train_weight)\n",
    "            importances = model.get_booster().get_score(importance_type='weight')\n",
    "            importances_df=pd.DataFrame(list(importances.items()), columns=['Key', 'Value'])\n",
    "            importances_df=importances_df.rename(columns={'Value': 'Value_%d'%i})\n",
    "            rel_con=rel_con.merge(importances_df, left_on='Key', right_on='Key', how='left')\n",
    "    rel_con['sum'] = rel_con.iloc[:, 1:].sum(axis=1)\n",
    "    rel_con['rc']=rel_con['sum']/rel_con['sum'].sum()\n",
    "    key_risk_df=rel_con[rel_con['rc']>=0.02].reset_index(drop=True)\n",
    "    key_risk_factors=key_risk_df['Key'].values\n",
    "    \n",
    "    #step 2, predict use key risk factors\n",
    "    rel_con=pd.DataFrame({'Key': key_risk_factors})\n",
    "    preds=[]\n",
    "    auc_value=[]\n",
    "    tpr_value=[]\n",
    "    tnr_value=[]\n",
    "    high_risk=[]\n",
    "    for i in range(100):\n",
    "        train, test = train_test_split(survey_data, test_size=0.25,  random_state=i)\n",
    "        while test[test[tick]==1].shape[0]<1 or train[train[tick]==1].shape[0]<1:\n",
    "            train, test = train_test_split(survey_data, test_size=0.25)\n",
    "\n",
    "        train_x = train[key_risk_factors]\n",
    "        train_y = train[tick]\n",
    "        train_weight =  train['rescaled']\n",
    "        model = xgb.XGBClassifier(objective='binary:logistic', learning_rate=given_learningrate, max_depth=given_treecomplexity, subsample=given_subsample, n_estimators=3000,  random_state=i) # colsample_bytree=1\n",
    "        model.fit(train_x, train_y, sample_weight=train_weight)\n",
    "        importances = model.get_booster().get_score(importance_type='weight')\n",
    "        importances_df=pd.DataFrame(list(importances.items()), columns=['Key', 'Value'])\n",
    "        importances_df=importances_df.rename(columns={'Value': 'Value_%d'%i})\n",
    "        rel_con=rel_con.merge(importances_df, left_on='Key', right_on='Key', how='left')\n",
    "        \n",
    "        test_x = test[key_risk_factors]\n",
    "        test_y = test[tick]\n",
    "        pred_test_y=model.predict_proba(test_x)[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(test_y, pred_test_y)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_value.append(roc_auc)\n",
    "        optimal_idx = np.argmax(tpr - fpr)  # 找到最大约登指数对应的索引\n",
    "        optimal_threshold = thresholds[optimal_idx]  # 最佳阈值\n",
    "        optimal_fpr = fpr[optimal_idx]  # 最佳阈值对应的FPR\n",
    "        optimal_tpr = tpr[optimal_idx]  # 最佳阈值对应的TPR\n",
    "        tpr_value.append(optimal_tpr)\n",
    "        tnr_value.append(1-optimal_fpr)\n",
    "        # optimal_threshold = thresholds[np.argmax(tpr - fpr)]          \n",
    "\n",
    "        all_x=complete_data[key_risk_factors]\n",
    "        pred_all_y=model.predict_proba(all_x)[:,1]\n",
    "        pred_df=pd.DataFrame({'id':complete_data['id'], 'country': complete_data['country'], 'province': complete_data['province'], 'county':complete_data['county'], 'round': [i]*complete_data.shape[0], 'status':complete_data[tick],'pred':pred_all_y,'sensitivity':[optimal_tpr]*complete_data.shape[0],'specificity': [(1-optimal_fpr)]*complete_data.shape[0]})\n",
    "        preds.append(pred_df)\n",
    "\n",
    "        high_risk_data=pred_df[pred_df['pred']>=optimal_threshold].reset_index(drop=True)\n",
    "        high_risk_data['threshold']=[optimal_threshold]*high_risk_data.shape[0]\n",
    "        high_risk.append(high_risk_data)\n",
    "\n",
    "    #mean auc of testing set\n",
    "    auc_mean=np.mean(auc_value)\n",
    "    auc_ubd=np.percentile(auc_value, 97.5)\n",
    "    auc_lbd=np.percentile(auc_value, 2.5)\n",
    "    auc_df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd'], 'value': [auc_mean, auc_lbd, auc_ubd]})\n",
    "    auc_df.to_excel(r'output/auc_%s.xlsx'%tick, index=False)\n",
    "\n",
    "    tpr_mean=np.mean(tpr_value)\n",
    "    tpr_ubd=np.percentile(tpr_value, 97.5)\n",
    "    tpr_lbd=np.percentile(tpr_value, 2.5)\n",
    "    tpr_df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd'], 'value': [tpr_mean, tpr_lbd, tpr_ubd]})\n",
    "    tpr_df.to_excel(r'output/tpr_%s.xlsx'%tick, index=False)\n",
    "\n",
    "    tnr_mean=np.mean(tnr_value)\n",
    "    tnr_ubd=np.percentile(tnr_value, 97.5)\n",
    "    tnr_lbd=np.percentile(tnr_value, 2.5)\n",
    "    tnr_df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd'], 'value': [tnr_mean, tnr_lbd, tnr_ubd]})\n",
    "    tnr_df.to_excel(r'output/tnr_%s.xlsx'%tick, index=False)\n",
    "\n",
    "\n",
    "    #relative contribution\n",
    "    rel_con['row_sum']=rel_con.iloc[:, 1:].sum(axis=1)\n",
    "    rel_con['std_sum']=rel_con['row_sum']/rel_con['row_sum'].sum()\n",
    "    rel_con.to_excel(r'output/rc_%s.xlsx'%tick, index=False)\n",
    "    \n",
    "    #high_risk_counties\n",
    "    high_risk_df=pd.concat(high_risk, ignore_index=True)\n",
    "    high_risk_df.to_excel(r'output/hrc_%s.xlsx'%tick, index=False)\n",
    "\n",
    "    #calcualte mean, std and 95% CI\n",
    "    pred_df=pd.concat(preds, ignore_index=True)\n",
    "    grouped = pred_df.groupby(['id', 'country', 'province', 'county', 'status'])['pred'].mean()\n",
    "    grouped = grouped.reset_index()\n",
    "    grouped.to_excel(r'output/predict_prob_%s.xlsx'%tick, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dermacentor marginatus 7 0.001 0.7\n"
     ]
    }
   ],
   "source": [
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "complexitys=[7, 7, 5, 5]\n",
    "learningrates=[0.001, 0.001, 0.001, 0.01]\n",
    "subsamples=[0.7, 0.7, 0.7, 0.8]\n",
    "for tick, complexity, learningrate, subsample in zip(ticks, complexitys, learningrates, subsamples):\n",
    "    print(tick, complexity, learningrate, subsample)\n",
    "    predict(tick, complexity, learningrate, subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd']})\n",
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "for i in range(len(ticks)):\n",
    "    tmp=pd.read_excel(r'output/auc_%s.xlsx'%ticks[i])\n",
    "    df[ticks[i]]=tmp['value']\n",
    "dfn=df.T\n",
    "dfn.to_excel(r'output/AUC_tick.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd']})\n",
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "for i in range(len(ticks)):\n",
    "    tmp=pd.read_excel(r'output/tpr_%s.xlsx'%ticks[i])\n",
    "    df[ticks[i]]=tmp['value']\n",
    "dfn=df.T\n",
    "dfn.to_excel(r'output/TPR_tick.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'index': ['mean', 'lbd', 'ubd']})\n",
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "for i in range(len(ticks)):\n",
    "    tmp=pd.read_excel(r'output/tnr_%s.xlsx'%ticks[i])\n",
    "    df[ticks[i]]=tmp['value']\n",
    "dfn=df.T\n",
    "dfn.to_excel(r'output/TNR_tick.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative contribution\n",
    "risk_factor=pd.read_excel(r'output/risk_factor/tick.xlsx')\n",
    "df=pd.DataFrame({'factor': risk_factor.columns[4:]})\n",
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "for i in range(len(ticks)):\n",
    "    tmp=pd.read_excel(r'output/rc_%s.xlsx'%ticks[i])\n",
    "    tmp=tmp[['Key', 'std_sum']]\n",
    "    tmp=tmp.rename(columns={'Key': 'factor', 'std_sum': ticks[i]})\n",
    "    df=df.merge(tmp, left_on='factor', right_on='factor', how='left')\n",
    "df.to_excel(r'output/RC_tick.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dermacentor marginatus 809\n",
      "Dermacentor nuttalli 1069\n",
      "Dermacentor silvarum 1154\n",
      "Hyalomma asiaticum 890\n"
     ]
    }
   ],
   "source": [
    "ticks=['Dermacentor marginatus', 'Dermacentor nuttalli', 'Dermacentor silvarum',  'Hyalomma asiaticum']\n",
    "for i in range(len(ticks)):\n",
    "    #high_risk_county, scenario 1\n",
    "    county_list=pd.read_excel(r'input/tick_risk_factor.xlsx')\n",
    "    county_list=county_list[['id', 'country', 'province', 'county']]\n",
    "    county_list['id']=county_list['id'].astype(str)\n",
    "    county_list['country']=county_list['country'].astype(str)\n",
    "    county_list['province']=county_list['province'].astype(str)\n",
    "    county_list['county']=county_list['county'].astype(str)\n",
    "\n",
    "    tick_data=pd.read_excel(r'output/hrc_%s.xlsx'%ticks[i])\n",
    "    tick_data['id']=tick_data['id'].astype(str)\n",
    "    tick_data['country']=tick_data['country'].astype(str)\n",
    "    tick_data['province']=tick_data['province'].astype(str)\n",
    "    tick_data['county']=tick_data['county'].astype(str)\n",
    "    for j in range(100):\n",
    "        tmp=tick_data[tick_data['round']==j].reset_index(drop=True)\n",
    "        new_column='flag_%d'%j\n",
    "        tmp[new_column]=[1]*tmp.shape[0]\n",
    "        tmp=tmp[['id','country','province','county', new_column]]\n",
    "        county_list=county_list.merge(tmp, left_on=['id','country','province','county'], right_on=['id','country','province','county'], how='left')\n",
    "    county_list.fillna(0, inplace=True)\n",
    "    county_list['status']=county_list.iloc[:, 4:].sum(axis=1)\n",
    "    df=county_list[county_list['status']>0].reset_index(drop=True)\n",
    "    df=df[['id','country','province','county', 'status']]\n",
    "    # df.rename(columns={'status': ticks[i]}, inplace=True)\n",
    "    df.to_excel(r'output/hrc_clean_%s.xlsx'%ticks[i], index=False)\n",
    "    print(ticks[i], df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dermacentor marginatus 81 809 898.7654320987655\n",
      "Dermacentor nuttalli 120 1069 790.8333333333334\n",
      "Dermacentor silvarum 66 1154 1648.4848484848485\n",
      "Hyalomma asiaticum 122 890 629.5081967213115\n"
     ]
    }
   ],
   "source": [
    "obv_number=[]\n",
    "obv_area=[]\n",
    "obv_pop=[]\n",
    "pred_number=[]\n",
    "pred_area=[]\n",
    "pred_pop=[]\n",
    "col_tick=[]\n",
    "for tick in ticks:\n",
    "    pred=pd.read_excel(r'output/hrc_clean_%s.xlsx'%tick)\n",
    "    pred['id']=pred['id'].astype(str)\n",
    "    pred['country']=pred['country'].astype(str)\n",
    "    pred['province']=pred['province'].astype(str)\n",
    "    pred['county']=pred['county'].astype(str)\n",
    "\n",
    "    obv=pd.read_excel(r'input/tick_county_distribution.xlsx')\n",
    "    obv=obv[['id', 'country', 'province', 'county', tick]]\n",
    "    obv=obv[obv[tick]>=1].reset_index(drop=True)\n",
    "    obv['id']=obv['id'].astype(str)\n",
    "    obv['country']=obv['country'].astype(str)\n",
    "    obv['province']=obv['province'].astype(str)\n",
    "    obv['county']=obv['county'].astype(str)\n",
    "    \n",
    "    county_information=pd.read_excel(r'input/population.xlsx')\n",
    "    county_information['population']=county_information['area (km²)']*county_information['population density (/km²)']\n",
    "    county_information['population']=county_information['population'].astype('int')\n",
    "    county_information['id']=county_information['id'].astype(str)\n",
    "    county_information['country']=county_information['country'].astype(str)\n",
    "    county_information['province']=county_information['province'].astype(str)\n",
    "    county_information['county']=county_information['county'].astype(str)\n",
    "\n",
    "    number=0\n",
    "    area=0\n",
    "    population=0\n",
    "    for j in range(pred.shape[0]):\n",
    "        id=pred['id'][j]\n",
    "        country=pred['country'][j]\n",
    "        province=pred['province'][j]\n",
    "        county=pred['county'][j]\n",
    "        tmp=county_information[(county_information['id']==id) & (county_information['country']==country) & (county_information['province']==province) & (county_information['county']==county)].reset_index(drop=True)\n",
    "        if tmp.shape[0]!=1:\n",
    "            print('pred', tmp)\n",
    "        else:\n",
    "            number+=1\n",
    "            area+=(tmp['area (km²)'][0])\n",
    "            population+=(tmp['population'][0])\n",
    "    pred_number.append(number)\n",
    "    pred_area.append(area)\n",
    "    pred_pop.append(population)\n",
    "\n",
    "    number=0\n",
    "    area=0\n",
    "    population=0\n",
    "    for j in range(obv.shape[0]):\n",
    "        id=obv['id'][j]\n",
    "        country=obv['country'][j]\n",
    "        province=obv['province'][j]\n",
    "        county=obv['county'][j]\n",
    "        tmp=county_information[(county_information['id']==id) & (county_information['country']==country) & (county_information['province']==province) & (county_information['county']==county)].reset_index(drop=True)\n",
    "        if tmp.shape[0]!=1:\n",
    "            print('pred', tmp)\n",
    "        else:\n",
    "            number+=1\n",
    "            area+=(tmp['area (km²)'][0])\n",
    "            population+=(tmp['population'][0])\n",
    "    obv_number.append(number)\n",
    "    obv_area.append(area)\n",
    "    obv_pop.append(population)   \n",
    "\n",
    "    col_tick.append(tick)\n",
    "\n",
    "col_1=[]\n",
    "col_2=[]\n",
    "col_3=[]\n",
    "for i in range(len(col_tick)):\n",
    "    obv1=obv_number[i]\n",
    "    pred1=pred_number[i]\n",
    "    ratio1=100*(pred1-obv1)/obv1\n",
    "    print(col_tick[i], obv1, pred1, ratio1)\n",
    "    col_1.append(\"%d/%d (%.1f)\"%(pred1, obv1, ratio1))\n",
    "\n",
    "    obv2=obv_area[i]/1e6\n",
    "    pred2=pred_area[i]/1e6\n",
    "    ratio2=100*(pred2-obv2)/obv2\n",
    "    col_2.append(\"%.2f/%.2f (%.1f)\"%(pred2, obv2, ratio2))\n",
    "\n",
    "    obv3=obv_pop[i]/1e6\n",
    "    pred3=pred_pop[i]/1e6\n",
    "    ratio3=100*(pred3-obv3)/obv3\n",
    "    col_3.append(\"%2.f/%.2f (%.1f)\"%(pred3, obv3, ratio3))\n",
    "\n",
    "dfn=pd.DataFrame({'tick': col_tick, 'county': col_1, 'area': col_2, 'population': col_3})\n",
    "dfn.to_excel(r'output/PC_tick.xlsx', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
